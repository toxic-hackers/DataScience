{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "J5n6hxVGIUot",
    "outputId": "fb02d9cd-56b2-42c2-f3a8-ff9d3b2d3081"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import html\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s4Q0dgoe_uBW"
   },
   "source": [
    "# Drop, Clean & Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "1xzdhyTS_3F9",
    "outputId": "d47f987b-7002-42b6-90f3-4e609e6e847a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/hn_all_w_sentiment_cleaned_inplace.csv\")\n",
    "df.head()\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15397309, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"commentid\",\"cleaned_comment\"]]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df = df.dropna(axis=0, how='any', thresh=None, subset=None, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commentid          0\n",
       "cleaned_comment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.fillna(\"NULL VALUE PLACEHOLDER COMMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "commentid          0\n",
       "cleaned_comment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data4’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a //10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = df.shape\n",
    "inc = a //10\n",
    "\n",
    "c = inc*1\n",
    "d = inc*2\n",
    "e = inc*3\n",
    "f = inc*4\n",
    "g = inc*5\n",
    "h = inc*6\n",
    "i = inc*7\n",
    "j = inc*8\n",
    "k = inc*9\n",
    "l = inc*10\n",
    "\n",
    "df_a = df[0:c]\n",
    "df_b = df[c:d]\n",
    "df_c = df[d:e]\n",
    "df_d = df[e:f]\n",
    "df_e = df[f:g]\n",
    "df_f = df[g:h]\n",
    "df_g = df[h:i]\n",
    "df_h = df[i:j]\n",
    "df_i = df[j:k]\n",
    "df_j = df[k:l]\n",
    "df_k = df[l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.to_csv('data4/hn_cleaned_ready_for_bert_a_11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in [c,d,e,f,g,h,i,j,k,l]:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.to_csv('data4/hn_cleaned_ready_for_bert_a1.csv',index=False)\n",
    "df_b.to_csv('data4/hn_cleaned_ready_for_bert_a2.csv',index=False)\n",
    "df_c.to_csv('data4/hn_cleaned_ready_for_bert_a3.csv',index=False)\n",
    "df_d.to_csv('data4/hn_cleaned_ready_for_bert_a11.csv',index=False)\n",
    "df_e.to_csv('data4/hn_cleaned_ready_for_bert_a4.csv',index=False)\n",
    "df_f.to_csv('data4/hn_cleaned_ready_for_bert_a5.csv',index=False)\n",
    "df_g.to_csv('data4/hn_cleaned_ready_for_bert_a6.csv',index=False)\n",
    "df_h.to_csv('data4/hn_cleaned_ready_for_bert_a7.csv',index=False)\n",
    "df_i.to_csv('data4/hn_cleaned_ready_for_bert_a8.csv',index=False)\n",
    "df_j.to_csv('data4/hn_cleaned_ready_for_bert_a9.csv',index=False)\n",
    "df_k.to_csv('data4/hn_cleaned_ready_for_bert_a10.csv',index=False)\n",
    "df_l.to_csv('data4/hn_cleaned_ready_for_bert_a11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—1234567890'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_processing = 1\n",
    "# for each_df in list_of_dfs:\n",
    "#     each_df['cleaned_comment'] = each_df[\"cleaned_comment\"].apply(lambda x: text.text_to_word_sequence(x, filters=CHARS_TO_REMOVE, lower=True, split=' '))\n",
    "#     print(\"Processed Sub_df # %s\" % df_processing)\n",
    "#     df_processing += 1\n",
    "df['cleaned_comments'] = df[\"cleaned_comment\"].apply(lambda x: text.text_to_word_sequence(x, filters=CHARS_TO_REMOVE, lower=True, split=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    return df\n",
    "\n",
    "df = standardize_text(df, 'cleaned_comment')\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['cleaned_comment'] = df[\"cleaned_comment\"].apply(lambda x: text.text_to_word_sequence(x, filters=CHARS_TO_REMOVE, lower=True, split=' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaned_words = [word for tokens in df['SentimentText'] for word in tokens]\n",
    "cleanedVOCAB = sorted(list(set(cleaned_words)))\n",
    "display(\"%s words total, with a vocabulary size of %s\" % (len(cleaned_words), len(cleanedVOCAB)))\n",
    "\n",
    "display(df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3iUeBKtG_uEK"
   },
   "source": [
    "# Part 3 - Document Classification\n",
    "\n",
    "1) Use Train_Test_Split to create train and test datasets.\n",
    "\n",
    "2) Vectorize the tokenized documents using your choice of vectorization method. \n",
    "\n",
    " - Stretch goal: Use both of the methods that we talked about in class.\n",
    "\n",
    "3) Create a vocabulary using the X_train dataset and transform both your X_train and X_test data using that vocabulary.\n",
    "\n",
    "4) Use your choice of binary classification algorithm to train and evaluate your model's accuracy. Report both train and test accuracies.\n",
    "\n",
    " - Stretch goal: Use an error metric other than accuracy and implement/evaluate multiple classifiers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gj0uXB0ebD5B"
   },
   "source": [
    "## Bag of Words w/ Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHP6Lsq3bM59"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TX8OEgUP_3ee"
   },
   "outputs": [],
   "source": [
    "# Defining the Count Vectorizer function. \n",
    "def cv(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "# Here I define my corpus and lables. These are used throughout all the other models/processes.\n",
    "list_corpus = df[\"SentimentText\"].apply(lambda y: \" \".join(str(x) for x in y)).tolist()\n",
    "list_labels = df[\"Sentiment\"].tolist()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n",
    "                                                                                random_state=seed)\n",
    "\n",
    "X_train_counts, count_vectorizer = cv(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "puQvju3WbClz"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=40)\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "y_predicted_counts = clf.predict(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7YRLWgMziwE3",
    "outputId": "c3906114-2eee-4b5e-8ce1-9b79ee020408"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_counts)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1sAfftg-kUzQ",
    "outputId": "724407b9-2d83-47b5-dbd6-3abda1a785d6"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "MNB = MultinomialNB().fit(X_train_counts, y_train)\n",
    "\n",
    "train_predictions = MNB.predict(X_train_counts)\n",
    "test_predictions = MNB.predict(X_test_counts)\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(y_test, test_predictions)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "csIkYNQQjfyQ"
   },
   "source": [
    "## Now BoW TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQyCqZvtjEDR"
   },
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDoskDcnjDqA"
   },
   "outputs": [],
   "source": [
    "clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                         multi_class='multinomial', n_jobs=-1, random_state=40)\n",
    "clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "um6uo-_bjI6v",
    "outputId": "71e2d6a0-2b01-4705-9da3-671c80735ebd"
   },
   "outputs": [],
   "source": [
    "accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy_tfidf, precision_tfidf, \n",
    "                                                                       recall_tfidf, f1_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sorF95UO_uGx"
   },
   "source": [
    "# Part 4 -  Word2Vec\n",
    "\n",
    "1) Fit a Word2Vec model on your cleaned/tokenized twitter dataset. \n",
    "\n",
    "2) Display the 10 words that are most similar to the word \"twitter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v7fxFzVHm8iW"
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYno4d4N-LHR"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(df[\"SentimentText\"].tolist(), min_count=1, size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "9MAit-0GmDvg",
    "outputId": "7605958f-0077-4742-ac88-6507535c8e94"
   },
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(list(model.wv.vocab))\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "AhFnqLlzmMJe",
    "outputId": "b56938af-dcb0-4542-9501-0bb2dfbde11f"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar('twitter')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS42SC.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
